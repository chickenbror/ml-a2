{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e157e7e",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "I made changes to the read_data and convert_to_index functions from the demo, so that the train/test datasets are represented as tuples of (x,y) (so that they can be batch-loaded using DataLoaader), with paddings and start/end tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8746f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27560932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chinese_data(inputfilename):\n",
    "    '''\n",
    "    Reads the input file, returns a list of [(x,y)...], \n",
    "    where x = a Chinese sentence string, y = a binary list [1,0...],\n",
    "    indicating whether a character is the first of a Chinese word.\n",
    "    Eg, (\"這次遊行的特色\", [1, 1, 1, 0, 1, 1, 0,]) for 這/次/遊行/的/特色\n",
    "    '''\n",
    "    with open(inputfilename, \"r\") as inputfile:\n",
    "        xy_list, collection_words, collection_labels = [], [], []\n",
    "        for line in inputfile:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            columns = line.split()\n",
    "            if columns == []: # When reading a blank line\n",
    "                xy_list.append((''.join(collection_words), collection_labels)) # append the (x,y)\n",
    "                collection_words, collection_labels = [], [] # Reset the x,y lists\n",
    "                continue\n",
    "            collection_words.append(columns[1])\n",
    "            collection_labels += [1] + ([0] * (len(columns[1]) - 1)) # 1 for first char, 0 for the rest\n",
    "    return xy_list\n",
    "\n",
    "def get_chars_and_ids(sentences, extra_chars=['<PAD>','<START>','<END>']):\n",
    "    '''\n",
    "    Arg:\n",
    "        sentences: a list of sentence-strings\n",
    "    Returns:\n",
    "        list of the character set, plus extras (default: pad, start & end tags), \n",
    "        dict of {character:char_id}, tag-and-id by default <PAD>:0, <START>:1, <END>:2\n",
    "    '''\n",
    "    char_set = set((char for sen in sentences for char in sen))\n",
    "    char_list = extra_chars + list(char_set)\n",
    "    ids_dict = {char:i for i,char in enumerate(char_list)}\n",
    "    return char_list, ids_dict\n",
    "\n",
    "def sentence_to_ids(sentence, ids_dict, add_tags=True, padding_len=512): \n",
    "    '''Turns a sentence-string into a [ids] array, adds start/end tag by default'''\n",
    "    \n",
    "    ids = np.array([ids_dict[char] for char in sentence]) # string to ids\n",
    "    \n",
    "    if add_tags and ('<START>' in ids_dict) and ('<END>' in ids_dict):\n",
    "        start_id, end_id = ids_dict['<START>'], ids_dict['<END>']\n",
    "        ids = np.pad(ids, (1, 1), 'constant', constant_values=(start_id, end_id)) # pad with start/end tags\n",
    "    \n",
    "    pad_id = ids_dict['<PAD>'] if '<PAD>' in ids_dict else len(ids_dict)+1 # pad_id or vocabsize+1\n",
    "    paddings = np.repeat(pad_id, padding_len - len(ids)+1) # Make sure even the longest sen has one padding\n",
    "    ids = np.concatenate((ids, paddings))\n",
    "    return ids\n",
    "\n",
    "def convert_and_pad(raw_xy_data, ids_dict, y_pad_id=-1):\n",
    "    '''Turns a list of (sentence, labels) to (padded_sentence_ids, padded_labels)'''\n",
    "    \n",
    "    max_len = max((len(x) for x,y in raw_xy_data))+2 # num_chars + 2 tags\n",
    "    id_and_pad = lambda x : sentence_to_ids(x, ids_dict=ids_dict, padding_len=max_len)\n",
    "    add_tag_and_pad_y = lambda y : np.concatenate( \n",
    "                                        (np.pad(y, 1, 'constant', constant_values=1), # label <start>/<end> as 1\n",
    "                                         np.repeat(y_pad_id, max_len-len(y)-1) # pads: max_len - len(y)- 2 tags +1\n",
    "                                        ))\n",
    "    \n",
    "    ided_and_padded = ((id_and_pad(x), np.array(len(x)+2), add_tag_and_pad_y(y)) for x,y in raw_xy_data)\n",
    "    return list(ided_and_padded) # (tagged_and_padded_x, len_x_plus_two_tags, padded_y_plus_two_more_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e570ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xy_raw = read_chinese_data('/scratch/lt2316-h20-resources/zh_gsd-ud-train.conllu')\n",
    "test_xy_raw = read_chinese_data('/scratch/lt2316-h20-resources/zh_gsd-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e74625d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>看似簡單，只是二選一做決擇，但其實他們代表的是你周遭的親朋好友，試著給你不同的意見，但追根究...</td>\n",
       "      <td>[1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>其便當都是買來的，就算加熱也是由媽媽負責（後來揭曉其實是避免帶來厄運），父親則在電視台上班。</td>\n",
       "      <td>[1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>這次遊行最大的特色，在於越來越多年輕人上街遊行，而且當中不乏行動激烈的躁少年。</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>懷孕期為421至457日。</td>\n",
       "      <td>[1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>婷婷向昏迷中的婆婆訴說，為什麼生活會與她想像的不一樣。</td>\n",
       "      <td>[1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   x  \\\n",
       "0  看似簡單，只是二選一做決擇，但其實他們代表的是你周遭的親朋好友，試著給你不同的意見，但追根究...   \n",
       "1     其便當都是買來的，就算加熱也是由媽媽負責（後來揭曉其實是避免帶來厄運），父親則在電視台上班。   \n",
       "2            這次遊行最大的特色，在於越來越多年輕人上街遊行，而且當中不乏行動激烈的躁少年。   \n",
       "3                                      懷孕期為421至457日。   \n",
       "4                        婷婷向昏迷中的婆婆訴說，為什麼生活會與她想像的不一樣。   \n",
       "\n",
       "                                                   y  \n",
       "0  [1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, ...  \n",
       "1  [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, ...  \n",
       "2  [1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, ...  \n",
       "3            [1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]  \n",
       "4  [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(train_xy_raw, columns=['x','y'])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf3e1cfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>x_len</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1289, 1477, 1724, 3511, 128, 2476, 1233, 8...</td>\n",
       "      <td>60</td>\n",
       "      <td>[1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 1640, 2879, 919, 3021, 1233, 329, 625, 249...</td>\n",
       "      <td>48</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 1349, 2371, 3034, 1605, 2805, 835, 2495, 8...</td>\n",
       "      <td>41</td>\n",
       "      <td>[1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 584, 643, 1515, 553, 1703, 567, 66, 37, 17...</td>\n",
       "      <td>15</td>\n",
       "      <td>[1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 2292, 2292, 920, 2793, 75, 1953, 2495, 356...</td>\n",
       "      <td>29</td>\n",
       "      <td>[1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   x x_len  \\\n",
       "0  [1, 1289, 1477, 1724, 3511, 128, 2476, 1233, 8...    60   \n",
       "1  [1, 1640, 2879, 919, 3021, 1233, 329, 625, 249...    48   \n",
       "2  [1, 1349, 2371, 3034, 1605, 2805, 835, 2495, 8...    41   \n",
       "3  [1, 584, 643, 1515, 553, 1703, 567, 66, 37, 17...    15   \n",
       "4  [1, 2292, 2292, 920, 2793, 75, 1953, 2495, 356...    29   \n",
       "\n",
       "                                                   y  \n",
       "0  [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, ...  \n",
       "1  [1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, ...  \n",
       "2  [1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, ...  \n",
       "3  [1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, ...  \n",
       "4  [1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sandwich x/y in start/end tags and pad to 184 long\n",
    "\n",
    "# set of all chars and their ids\n",
    "chars_list, ids_dict = get_chars_and_ids((x for x,y in train_xy_raw + test_xy_raw)) \n",
    "\n",
    "# Convert (sentence,labels) to (padded ids, len_padded_ids, padded labels)\n",
    "train_xy = convert_and_pad(train_xy_raw, ids_dict)\n",
    "test_xy = convert_and_pad(test_xy_raw, ids_dict)\n",
    "\n",
    "pd.DataFrame(train_xy, columns=['x','x_len','y'])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a7e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Segmenter(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        \n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_size, 0)\n",
    "        self.lstm = nn.LSTM(self.emb_size, 150, batch_first=True) # in_size, hidden_size, layers=1\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.lin = nn.Linear(150, 2)\n",
    "        self.softmax = nn.LogSoftmax(2)\n",
    "        \n",
    "    def forward(self, x, x_len):\n",
    "        embs = self.emb(x) # B,185,200\n",
    "        rnn_output, (_,_) = self.lstm(embs) # B, 185, 150\n",
    "        \n",
    "        # 150=>linear 2=>softmax\n",
    "        output = self.sig1(rnn_output) # B, 185, 150\n",
    "        output = self.lin(output) # B, 185, 150\n",
    "        output = self.softmax(output) # B, 185, 2\n",
    "        output = output[:, :max(x_len), :] # outshape B, max_x_len_of_batch(<=184), 2\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6356763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96dd71cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_xy, epochs, device, model, model_fn, batch_size=50, lr=0.005):\n",
    "    \n",
    "    m = model.to(device)\n",
    "    m.train()\n",
    "    \n",
    "    batching = DataLoader(train_xy, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    loss_fn = nn.NLLLoss(ignore_index=-1) # ignore y padding\n",
    "    optimizer = optim.Adam(m.parameters(), lr=0.005)\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i, (x, x_len, y) in enumerate(batching):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            out = m(x, x_len) # B, max_x_len, 2 => permute to B,2,max_x_len\n",
    "            expect = y[:, :max(x_len)] # B, max_x_len\n",
    "            \n",
    "            loss = loss_fn(out.permute(0,2,1), expect) \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(f\"Epoch {e+1} avg loss {total_loss/(i+1)}\", end='\\r')\n",
    "        print()\n",
    "        torch.save(m, model_fn)\n",
    "\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2086c045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg loss 0.38333669602870946\n",
      "Epoch 2 avg loss 0.20807320140302182\n",
      "Epoch 3 avg loss 0.15759735815227033\n",
      "Epoch 4 avg loss 0.12261073337867856\n",
      "Epoch 5 avg loss 0.09514971654862166\n",
      "Epoch 6 avg loss 0.07417137511074542\n",
      "Epoch 7 avg loss 0.057458917796611795\n",
      "Epoch 8 avg loss 0.046411604667082426\n",
      "Epoch 9 avg loss 0.036904990044422456\n",
      "Epoch 10 avg loss 0.029521618806757032\n",
      "Epoch 11 avg loss 0.026479475363157693\n",
      "Epoch 12 avg loss 0.027870772918686275\n",
      "Epoch 13 avg loss 0.023977186786942183\n",
      "Epoch 14 avg loss 0.019785709120333193\n",
      "Epoch 15 avg loss 0.021285586035810412\n",
      "Epoch 16 avg loss 0.016378332156455143\n",
      "Epoch 17 avg loss 0.014341231202706695\n",
      "Epoch 18 avg loss 0.011373420292511583\n",
      "Epoch 19 avg loss 0.010060079692630097\n",
      "Epoch 20 avg loss 0.0074300358886830505\n",
      "Epoch 21 avg loss 0.0047712409170344476\n",
      "Epoch 22 avg loss 0.0028920972792548126\n",
      "Epoch 23 avg loss 0.0019565121176128743\n",
      "Epoch 24 avg loss 0.0016522447862371338\n",
      "Epoch 25 avg loss 0.0012530678715847899\n",
      "Epoch 26 avg loss 0.0010806785488966853\n",
      "Epoch 27 avg loss 0.0009314351522334618\n",
      "Epoch 28 avg loss 0.0008417397079028887\n",
      "Epoch 29 avg loss 0.0007804270302585792\n",
      "Epoch 30 avg loss 0.00077415470659616414\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(ids_dict)\n",
    "model_A_init = Segmenter(vocab_size=vocab_size, emb_size=200)\n",
    "gpu = 'cuda:3'\n",
    "\n",
    "model_A = train_model(train_xy, epochs=30, device=gpu, model=model_A_init, model_fn='model_A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88720216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Segmenter(\n",
       "  (emb): Embedding(3650, 200, padding_idx=0)\n",
       "  (lstm): LSTM(200, 150, batch_first=True)\n",
       "  (sig1): Sigmoid()\n",
       "  (lin): Linear(in_features=150, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model class must be defined somewhere (no need to give model(*args))\n",
    "model_A = torch.load('model_A').to('cpu')\n",
    "model_A.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbfc6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_batching = DataLoader(test_xy, batch_size=10)\n",
    "    for i,(x,x_len,y) in enumerate(test_batching):\n",
    "        rawpredictions = model_A(x, x_len)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67515cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<START>',\n",
       "  '然',\n",
       "  '而',\n",
       "  '，',\n",
       "  '這',\n",
       "  '樣',\n",
       "  '的',\n",
       "  '處',\n",
       "  '理',\n",
       "  '也',\n",
       "  '衍',\n",
       "  '生',\n",
       "  '了',\n",
       "  '一',\n",
       "  '些',\n",
       "  '問',\n",
       "  '題',\n",
       "  '。',\n",
       "  '<END>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>'],\n",
       " ['<START>',\n",
       "  '自',\n",
       "  '從',\n",
       "  '2',\n",
       "  '0',\n",
       "  '0',\n",
       "  '4',\n",
       "  '年',\n",
       "  '提',\n",
       "  '出',\n",
       "  '了',\n",
       "  '興',\n",
       "  '建',\n",
       "  '人',\n",
       "  '文',\n",
       "  '大',\n",
       "  '樓',\n",
       "  '的',\n",
       "  '構',\n",
       "  '想',\n",
       "  '，',\n",
       "  '企',\n",
       "  '業',\n",
       "  '界',\n",
       "  '陸',\n",
       "  '續',\n",
       "  '有',\n",
       "  '人',\n",
       "  '提',\n",
       "  '供',\n",
       "  '捐',\n",
       "  '款',\n",
       "  '。',\n",
       "  '<END>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>'],\n",
       " ['<START>',\n",
       "  '杜',\n",
       "  '鵑',\n",
       "  '花',\n",
       "  '為',\n",
       "  '溫',\n",
       "  '帶',\n",
       "  '植',\n",
       "  '物',\n",
       "  '，',\n",
       "  '台',\n",
       "  '北',\n",
       "  '雖',\n",
       "  '然',\n",
       "  '在',\n",
       "  '亞',\n",
       "  '熱',\n",
       "  '帶',\n",
       "  '，',\n",
       "  '但',\n",
       "  '冬',\n",
       "  '季',\n",
       "  '的',\n",
       "  '東',\n",
       "  '北',\n",
       "  '季',\n",
       "  '風',\n",
       "  '卻',\n",
       "  '使',\n",
       "  '得',\n",
       "  '杜',\n",
       "  '鵑',\n",
       "  '花',\n",
       "  '在',\n",
       "  '臺',\n",
       "  '大',\n",
       "  '宜',\n",
       "  '然',\n",
       "  '自',\n",
       "  '得',\n",
       "  '。',\n",
       "  '<END>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>'],\n",
       " ['<START>',\n",
       "  '臺',\n",
       "  '大',\n",
       "  '醫',\n",
       "  '學',\n",
       "  '人',\n",
       "  '文',\n",
       "  '博',\n",
       "  '物',\n",
       "  '館',\n",
       "  '是',\n",
       "  '一',\n",
       "  '棟',\n",
       "  '兩',\n",
       "  '層',\n",
       "  '樓',\n",
       "  '的',\n",
       "  '建',\n",
       "  '築',\n",
       "  '，',\n",
       "  '沿',\n",
       "  '中',\n",
       "  '山',\n",
       "  '南',\n",
       "  '路',\n",
       "  '與',\n",
       "  '仁',\n",
       "  '愛',\n",
       "  '路',\n",
       "  '成',\n",
       "  'L',\n",
       "  '型',\n",
       "  '。',\n",
       "  '<END>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>'],\n",
       " ['<START>',\n",
       "  '樓',\n",
       "  '頂',\n",
       "  '有',\n",
       "  '天',\n",
       "  '文',\n",
       "  '台',\n",
       "  '，',\n",
       "  '現',\n",
       "  '為',\n",
       "  '天',\n",
       "  '文',\n",
       "  '社',\n",
       "  '使',\n",
       "  '用',\n",
       "  '。',\n",
       "  '<END>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>'],\n",
       " ['<START>',\n",
       "  '國',\n",
       "  '際',\n",
       "  '北',\n",
       "  '極',\n",
       "  '研',\n",
       "  '究',\n",
       "  '中',\n",
       "  '心',\n",
       "  '的',\n",
       "  '主',\n",
       "  '要',\n",
       "  '夥',\n",
       "  '伴',\n",
       "  '是',\n",
       "  '日',\n",
       "  '本',\n",
       "  '和',\n",
       "  '美',\n",
       "  '國',\n",
       "  '，',\n",
       "  '參',\n",
       "  '與',\n",
       "  '會',\n",
       "  '務',\n",
       "  '的',\n",
       "  '還',\n",
       "  '有',\n",
       "  '來',\n",
       "  '自',\n",
       "  '加',\n",
       "  '拿',\n",
       "  '大',\n",
       "  '、',\n",
       "  '中',\n",
       "  '國',\n",
       "  '、',\n",
       "  '丹',\n",
       "  '麥',\n",
       "  '、',\n",
       "  '德',\n",
       "  '國',\n",
       "  '、',\n",
       "  '日',\n",
       "  '本',\n",
       "  '、',\n",
       "  '挪',\n",
       "  '威',\n",
       "  '、',\n",
       "  '俄',\n",
       "  '羅',\n",
       "  '斯',\n",
       "  '、',\n",
       "  '英',\n",
       "  '國',\n",
       "  '和',\n",
       "  '美',\n",
       "  '國',\n",
       "  '的',\n",
       "  '代',\n",
       "  '表',\n",
       "  '。',\n",
       "  '<END>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>'],\n",
       " ['<START>',\n",
       "  '其',\n",
       "  '中',\n",
       "  '參',\n",
       "  '賽',\n",
       "  '者',\n",
       "  '年',\n",
       "  '齡',\n",
       "  '不',\n",
       "  '可',\n",
       "  '超',\n",
       "  '過',\n",
       "  '1',\n",
       "  '8',\n",
       "  '歲',\n",
       "  '（',\n",
       "  '以',\n",
       "  '當',\n",
       "  '年',\n",
       "  '7',\n",
       "  '月',\n",
       "  '1',\n",
       "  '日',\n",
       "  '為',\n",
       "  '準',\n",
       "  '）',\n",
       "  '，',\n",
       "  '且',\n",
       "  '必',\n",
       "  '須',\n",
       "  '就',\n",
       "  '讀',\n",
       "  '於',\n",
       "  '中',\n",
       "  '學',\n",
       "  '校',\n",
       "  '（',\n",
       "  'S',\n",
       "  'e',\n",
       "  'c',\n",
       "  'o',\n",
       "  'n',\n",
       "  'd',\n",
       "  'a',\n",
       "  'r',\n",
       "  'y',\n",
       "  'S',\n",
       "  'c',\n",
       "  'h',\n",
       "  'o',\n",
       "  'o',\n",
       "  'l',\n",
       "  '）',\n",
       "  '。',\n",
       "  '<END>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>'],\n",
       " ['<START>',\n",
       "  '同',\n",
       "  '年',\n",
       "  '9',\n",
       "  '月',\n",
       "  '7',\n",
       "  '日',\n",
       "  '，',\n",
       "  '亞',\n",
       "  '奧',\n",
       "  '理',\n",
       "  '事',\n",
       "  '會',\n",
       "  '主',\n",
       "  '席',\n",
       "  '薩',\n",
       "  '巴',\n",
       "  '赫',\n",
       "  '親',\n",
       "  '王',\n",
       "  '為',\n",
       "  '國',\n",
       "  '際',\n",
       "  '射',\n",
       "  '擊',\n",
       "  '中',\n",
       "  '心',\n",
       "  '主',\n",
       "  '持',\n",
       "  '銅',\n",
       "  '像',\n",
       "  '揭',\n",
       "  '幕',\n",
       "  '儀',\n",
       "  '式',\n",
       "  '。',\n",
       "  '<END>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>'],\n",
       " ['<START>',\n",
       "  '這',\n",
       "  '些',\n",
       "  '電',\n",
       "  '話',\n",
       "  '經',\n",
       "  '交',\n",
       "  '換',\n",
       "  '機',\n",
       "  '處',\n",
       "  '理',\n",
       "  '，',\n",
       "  '使',\n",
       "  '用',\n",
       "  '的',\n",
       "  '媒',\n",
       "  '介',\n",
       "  '包',\n",
       "  '括',\n",
       "  '海',\n",
       "  '底',\n",
       "  '電',\n",
       "  '纜',\n",
       "  '、',\n",
       "  '人',\n",
       "  '造',\n",
       "  '衛',\n",
       "  '星',\n",
       "  '、',\n",
       "  '無',\n",
       "  '線',\n",
       "  '電',\n",
       "  '、',\n",
       "  '光',\n",
       "  '纖',\n",
       "  '及',\n",
       "  'I',\n",
       "  'P',\n",
       "  '電',\n",
       "  '話',\n",
       "  '（',\n",
       "  'V',\n",
       "  'O',\n",
       "  'I',\n",
       "  'P',\n",
       "  '）',\n",
       "  '。',\n",
       "  '<END>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>'],\n",
       " ['<START>',\n",
       "  '《',\n",
       "  '圓',\n",
       "  '月',\n",
       "  '彎',\n",
       "  '刀',\n",
       "  '》',\n",
       "  '為',\n",
       "  '古',\n",
       "  '龍',\n",
       "  '晚',\n",
       "  '期',\n",
       "  '作',\n",
       "  '品',\n",
       "  '，',\n",
       "  '1',\n",
       "  '9',\n",
       "  '7',\n",
       "  '6',\n",
       "  '年',\n",
       "  '6',\n",
       "  '月',\n",
       "  '至',\n",
       "  '1',\n",
       "  '9',\n",
       "  '7',\n",
       "  '8',\n",
       "  '年',\n",
       "  '5',\n",
       "  '月',\n",
       "  '，',\n",
       "  '香',\n",
       "  '港',\n",
       "  '〈',\n",
       "  '武',\n",
       "  '俠',\n",
       "  '春',\n",
       "  '秋',\n",
       "  '〉',\n",
       "  '2',\n",
       "  '8',\n",
       "  '2',\n",
       "  '至',\n",
       "  '3',\n",
       "  '4',\n",
       "  '8',\n",
       "  '期',\n",
       "  '斷',\n",
       "  '續',\n",
       "  '連',\n",
       "  '載',\n",
       "  '，',\n",
       "  '原',\n",
       "  '名',\n",
       "  '《',\n",
       "  '刀',\n",
       "  '神',\n",
       "  '》',\n",
       "  '，',\n",
       "  '1',\n",
       "  '9',\n",
       "  '7',\n",
       "  '8',\n",
       "  '年',\n",
       "  '漢',\n",
       "  '麟',\n",
       "  '出',\n",
       "  '版',\n",
       "  '改',\n",
       "  '名',\n",
       "  '《',\n",
       "  '圓',\n",
       "  '月',\n",
       "  '彎',\n",
       "  '刀',\n",
       "  '》',\n",
       "  '。',\n",
       "  '<END>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ [chars_list[i] for i in sen_ids] for sen_ids in x.detach().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd430a9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1],\n",
       "        [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0],\n",
       "        [1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1],\n",
       "        [1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0],\n",
       "        [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1],\n",
       "        [1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "         0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0],\n",
       "        [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1],\n",
       "        [1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "         0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "         0, 1, 0, 1, 0, 1],\n",
       "        [1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "         1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 0, 1, 0],\n",
       "        [1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,\n",
       "         0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "         0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawpredictions.argmax(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc66a89",
   "metadata": {},
   "source": [
    "### Part 1 - Sentence generation (15 points).\n",
    "Convert the model in Demo 2.1 into a character-based sentence generator. (Strip out the word segmentation objective.)  The model should, given a start symbol, produce a variety of sentences that terminate with a stop symbol (you will have to add these to the data).  The sentences that it generates should be of reasonable average length compared to the sentences in the training corpus (this needn't be precise). \n",
    "\n",
    "Report and discuss the changes you made to the notebook using Markdown inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ba8b07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/\n",
    "class SentenceGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=200, lstm_size=150):\n",
    "        super(Sentence_generator, self).__init__()\n",
    "        self.lstm_size = lstm_size\n",
    "        self.emb_size = emb_size\n",
    "        self.num_layers = 1\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_size, padding_idx=0) #V=>200\n",
    "        \n",
    "        self.lstm = nn.LSTM( #200=>150\n",
    "            input_size=self.emb_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "#             dropout=0.2,\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(self.lstm_size, self.vocab_size) #150=>V\n",
    "\n",
    "    def forward(self, x, hidden_state):\n",
    "        embed = self.embedding(x) #B,184,200\n",
    "        output, hidden_state = self.lstm(embed)\n",
    "        logits = self.fc(output)\n",
    "        return logits, hidden_state\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.num_layers, batch_size, self.lstm_size).zero_(),\n",
    "                weight.new(self.num_layers, batch_size, self.lstm_size).zero_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a05fc53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_B(train_xy, model, epochs, device, model_fn, batch_size=50):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    batching = DataLoader(train_xy, batch_size=batch_size, shuffle=True)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        \n",
    "        for i, (x,x_len,_) in enumerate(batching):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            hidden = tuple([s.data for s in hidden])\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # output preditions and update hidden state\n",
    "            predictions, hidden = model(x[:,:max(x_len)], hidden) # B,max_x_len => B,max_x_len,V\n",
    "            expect = x[:, 1:max(x_len)+1] # next words\n",
    "            \n",
    "            loss = criterion(predictions.transpose(1, 2), expect)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            print(f\"epoch {e+1} avg loss: {total_loss/(i+1)}\", end='\\r')\n",
    "\n",
    "        print()\n",
    "        torch.save(model, model_fn)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "10810753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 avg loss: 6.8489125132560735\n",
      "epoch 2 avg loss: 6.1349561989307435\n",
      "epoch 3 avg loss: 5.8704470455646515\n",
      "epoch 4 avg loss: 5.6516322493553165\n",
      "epoch 5 avg loss: 5.4676197111606655\n",
      "epoch 6 avg loss: 5.3089046180248265\n",
      "epoch 7 avg loss: 5.1739695847034455\n",
      "epoch 8 avg loss: 5.0540517389774325\n",
      "epoch 9 avg loss: 4.9488528907299045\n",
      "epoch 10 avg loss: 4.8528389096260075\n",
      "epoch 11 avg loss: 4.7643896341323855\n",
      "epoch 12 avg loss: 4.6818116128444675\n",
      "epoch 13 avg loss: 4.6036221206188245\n",
      "epoch 14 avg loss: 4.5322993218898775\n",
      "epoch 15 avg loss: 4.4612283766269684\n",
      "epoch 16 avg loss: 4.3950440645217895\n",
      "epoch 17 avg loss: 4.3312052309513095\n",
      "epoch 18 avg loss: 4.2702874422073365\n",
      "epoch 19 avg loss: 4.2089480727911955\n",
      "epoch 20 avg loss: 4.1515483319759365\n",
      "epoch 21 avg loss: 4.0961863338947385\n",
      "epoch 22 avg loss: 4.0424893707036975\n",
      "epoch 23 avg loss: 3.9883403956890104\n",
      "epoch 24 avg loss: 3.9372513562440874\n",
      "epoch 25 avg loss: 3.8862759709358214\n",
      "epoch 26 avg loss: 3.8368305832147637\n",
      "epoch 27 avg loss: 3.7892545402050017\n",
      "epoch 28 avg loss: 3.7407387882471084\n",
      "epoch 29 avg loss: 3.6969841152429585\n",
      "epoch 30 avg loss: 3.6515444368124015\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(ids_dict)\n",
    "model_B_init = Sentence_generator(vocab_size=vocab_size)\n",
    "gpu = 'cuda:0'\n",
    "\n",
    "model_B = train_model_B(train_xy, model=model_B_init, epochs=30, device=gpu, model_fn='model_B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0ec07e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence_generator(\n",
       "  (embedding): Embedding(3650, 200, padding_idx=0)\n",
       "  (lstm): LSTM(200, 150, batch_first=True)\n",
       "  (fc): Linear(in_features=150, out_features=3650, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B = torch.load('model_B').to('cpu')\n",
    "model_B.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8f4c58a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pred_sentence(ids_dict, chars_list, model, text, consider_prev=0):\n",
    "    model.eval()\n",
    "\n",
    "    chars = text\n",
    "    hidden = model.init_hidden(1) #batchsize=1\n",
    "\n",
    "    while True:\n",
    "        # (if consider_prev=0 or greater than the nr of chars, the whole sentence so far is considered)\n",
    "        current_ids = [ids_dict[c] for c in chars[-consider_prev:]] \n",
    "        \n",
    "        x = torch.tensor([current_ids]).long() # Consider <start> ~ last predicted char\n",
    "        with torch.no_grad():\n",
    "            y_pred, hidden = model(x, hidden)\n",
    "\n",
    "        last_char_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_char_logits, dim=0).detach().numpy()\n",
    "        sample_char_index = np.random.choice(len(last_char_logits), p=p)\n",
    "        next_char = chars_list[sample_char_index]\n",
    "#         if next_char == '<PAD>':\n",
    "#             continue\n",
    "        chars.append(next_char)\n",
    "        if next_char=='<END>':\n",
    "            break\n",
    "\n",
    "    return chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c365bd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<START>', '這', '種', '和', '社', '芳', '了', '巨', '大', '滿', '，', '保', '核', '雲', '酸', '劑', '礦', '育', '被', '評', '為', '稱', '為', '歌', '地', '，', '也', '進', '不', '給', '目', '前', '，', '就', '西', '班', '牙', '殖', '民', '國', '富', '神', '派', '獎', '。', '<END>']\n"
     ]
    }
   ],
   "source": [
    "print( generate_pred_sentence(ids_dict, chars_list, model=model_B, text=['<START>','這']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "afe3dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_sentences(consider_prev, n_sentences = 20, print_sentences=False):\n",
    "\n",
    "    current_len = 0\n",
    "    for i in range(n_sentences):\n",
    "        words = generate_pred_sentence(ids_dict, chars_list, model=model_B, text=['<START>'],\n",
    "                                      consider_prev=consider_prev)\n",
    "        \n",
    "        if print_sentences:\n",
    "            print(''.join(words), len(words))\n",
    "        current_len+=len(words)\n",
    "    print('Avg sentence length:', current_len/n_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "42869134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consider the previous 0 characters:\tAvg sentence length: 27.0\n",
      "consider the previous 1 characters:\tAvg sentence length: 1341.65\n",
      "consider the previous 5 characters:\tAvg sentence length: 1153.0\n",
      "consider the previous 10 characters:\tAvg sentence length: 418.0\n",
      "consider the previous 20 characters:\tAvg sentence length: 70.75\n",
      "consider the previous 30 characters:\tAvg sentence length: 44.65\n",
      "consider the previous 60 characters:\tAvg sentence length: 49.1\n",
      "consider the previous 100 characters:\tAvg sentence length: 42.15\n",
      "consider the previous 150 characters:\tAvg sentence length: 38.0\n",
      "consider the previous 184 characters:\tAvg sentence length: 44.55\n"
     ]
    }
   ],
   "source": [
    "# Trying considering different previous nr of chars, and see the avg length of generated sentences\n",
    "for i in (0,1,5,10,20,30,60,100,150,184):\n",
    "    print(f'consider the previous {i} characters:', end='\\t')\n",
    "    generate_n_sentences(consider_prev=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e4ffa21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START>分整謂十現代榮譽事誕生在唐王希遺城的席會。<END> 23\n",
      "<START>在2005年8月2郡，安清朝政府為主人、美軍人運與周圍1500萬位。<END> 36\n",
      "<START>大戰期的「洲藝指出1日光棍節目客紀、賈瓷、密等等同反設有些長端猶力「綠譜長漂者」稱下，美洲女歌藝子溫解死統一，可以澳門前主要性盟屬於1狀但實帶東流、澳車變化和公言（英國玉的尖）對帕爾勳為使用病支演出李八役，於1942年他藉他火星的左委託上他，所以寶幕高45秦國元，為華東方有成半丹。<END> 143\n",
      "<START>8克電視列高由站盛頓全國指揮計工作的過資。<END> 23\n",
      "<START>這種蜥期時間的載劉作為人可以取得白色襲觀。<END> 23\n",
      "<START>新波蘭堡的自經開展合書記運動會合約，原本上將航水流變檢隱60被承人於2007年4月5日。<END> 46\n",
      "<START>1993年10月20日，旨車站第一任舞人，拔於1950年首全變保護單位。<END> 38\n",
      "<START>為了許多芬受的罵先後，被奇難小思化作了建立的劃動，分別就養由「泥塑安」在個人類軍傳一t。<END> 46\n",
      "<START>神廳還言電塔未轉移早位。<END> 14\n",
      "<START>1980年2月，故宮立國全國旗陳新國家立後歐·相投資子為青7，港登西班牙奪出家那舉行肖構我們工程的運作為，要求出來自白統射豐這面的效果，他們沒有受到照越天李回獨立黛研作。<END> 87\n",
      "<START>後來另一個人爲更明他的核心，但他們只會接著他的女兒和儀式劇。<END> 32\n",
      "<START>他的胡予划化堂，對當三本時與日本時期的收拾將軍足球觀藝完危。<END> 32\n",
      "<START>在普問體便收太，厭靈由不可以提鑄高愛或因大都遭到解。<END> 28\n",
      "<START>根據20077年人口87萬，GonipernagustertsbitellfAWore，核終無是假說「酸」以上規斯在上的翻唱，但我們已很一定貢增加菌益的成功。<END> 81\n",
      "<START>例如，與瑞典再基出系統與當時他們知識需要原來自由陸系列球的率宿效體，中國歷史心市是抗議，長至10到舊村。<END> 54\n",
      "<START>他往後早在知裡變成透高的所肯，使關長奧斯·馬爾將軍發達9%的飛行的面業會的指石。<END> 42\n",
      "<START>該市政權在墨西哥的數呼劇家合併，各角炮頂制。<END> 24\n",
      "<START>科技式物交設備基督及三等教區交響力素之一。<END> 23\n",
      "<START>浙江省河城的文物中，全會成功的真量。<END> 20\n",
      "<START>榜東部市宣布退役，相當這年測試各有差手工播試建造的，使其府申、最高、旦作、19、鄧季至15號線、和江蘇俄、西泊洋、泡光、沽渚等、浦村、印校、新琉等城坂之一、局心保持等、滯支。<END> 89\n",
      "Avg sentence length: 45.2\n"
     ]
    }
   ],
   "source": [
    "generate_n_sentences(consider_prev=0, print_sentences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1916086c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentence length in train set: 41.10357768326244\n"
     ]
    }
   ],
   "source": [
    "avg_sent_len = np.array([l for x,l,y in train_xy]).mean()\n",
    "print('Average sentence length in train set:', avg_sent_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59495270",
   "metadata": {},
   "source": [
    "### Part 2 - Dual objectives (10 points)\n",
    "Copy the notebook from part 1 and augment the copy by adding back the word segmentation objective, as a second objective with its own loss.  (You could also in theory do Part 1 and Part 2 in reverse, by adding sentence generation with dual objectives first and then stripping out the word segmentation objective; this is equivalent.)  Note that multiple losses can be combined by simple, possibly weighted addition -- backpropagation works entirely correctly on the combined loss.\n",
    "\n",
    "Report and discuss the changes you made to the notebook using Markdown inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d145227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(vocabsize)(x, h_state) => out1:[B,maxlen,2] ; out2:([B,maxlen-1,vocabsize], h_state)\n",
    "class DualModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=200, lstm_size=150):\n",
    "        super(DualModel, self).__init__()\n",
    "        # Model Params\n",
    "        self.lstm_size = lstm_size\n",
    "        self.emb_size = emb_size\n",
    "        self.num_layers = 1\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Shared functions: embed, LSTM\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_size, padding_idx=0) #V=>200\n",
    "        self.lstm = nn.LSTM( #200=>150\n",
    "            input_size=self.emb_size, hidden_size=self.lstm_size, num_layers=self.num_layers,\n",
    "            batch_first=True,  )\n",
    "        \n",
    "        # For output1: sigmoid, linear( h_size->2 ), softmax\n",
    "        self.sig1 = nn.Sigmoid()\n",
    "        self.lin_to_binary = nn.Linear(self.lstm_size, 2)\n",
    "        self.softmax = nn.LogSoftmax(2)\n",
    "        \n",
    "        # For output2: linear( h_size->voc_size )\n",
    "        self.lin_to_v = nn.Linear(self.lstm_size, self.vocab_size) #150=>V\n",
    "    \n",
    "    def forward(self, x, hidden_state):\n",
    "        # original x: B,185\n",
    "        \n",
    "        # shared: embedding & lstm\n",
    "        embed = self.embedding(x) # B,185,200\n",
    "        rnn_out, hidden_state = self.lstm(embed) # B,185,150 & h_state\n",
    "        \n",
    "        # output1\n",
    "        output1 = self.sig1(rnn_out) # B, 185, 150\n",
    "        output1 = self.lin_to_binary(output1) # B, 185, 2\n",
    "        output1 = self.softmax(output1) # B, 185, 2  \n",
    "            # To be sliced outside model to => output[:, :max(x_len), :], shape = B, maxlen, 2\n",
    "        \n",
    "        # output2\n",
    "        logits = self.lin_to_v(rnn_out) #B,185,V\n",
    "        output2 = (logits, hidden_state)\n",
    "        \n",
    "        return output1, output2\n",
    "        \n",
    "    #====================================================\n",
    "        \n",
    "    # Func to initilize a random hidden state  \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.num_layers, batch_size, self.lstm_size).zero_(),\n",
    "                weight.new(self.num_layers, batch_size, self.lstm_size).zero_())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c74b59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dualmodel(train_xy, model, epochs, device, model_fn, batch_size=50):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    batching = DataLoader(train_xy, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    cross_entropy_fn = nn.CrossEntropyLoss(ignore_index=0) # ignore sentence padding\n",
    "    nll_fn = nn.NLLLoss(ignore_index=-1) # ignore y padding\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        total_loss = 0\n",
    "        \n",
    "        hidden = model.init_hidden(batch_size) # the initial random hidden state\n",
    "        \n",
    "        for i, (x, x_len, y) in enumerate(batching):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            hidden = tuple([s.data for s in hidden])\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            out1, out2 = model(x[:,:max(x_len)], hidden)\n",
    "            \n",
    "            # output1 loss\n",
    "            out1 = out1[:, :max(x_len), :] # B, maxlen, 2\n",
    "            expect1 = y[:, :max(x_len)] # B, max_x_len\n",
    "            loss1 = nll_fn(out1.transpose(1, 2), expect1)\n",
    "            \n",
    "            # output2 loss\n",
    "            pred, hidden = out2\n",
    "            expect2 = x[:, 1:max(x_len)+1] # next words\n",
    "            loss2 = cross_entropy_fn(pred.transpose(1, 2), expect2)\n",
    "            \n",
    "\n",
    "            loss = loss1+loss2\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            print(f\"epoch {e+1} avg loss: {total_loss/(i+1)}\", end='\\r')\n",
    "\n",
    "        print()\n",
    "        torch.save(model, model_fn)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ec3ee98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 avg loss: 7.5126067101955415\n",
      "epoch 2 avg loss: 6.6897525012493135\n",
      "epoch 3 avg loss: 6.3669405341148374\n",
      "epoch 4 avg loss: 6.1060999870300375\n",
      "epoch 5 avg loss: 5.8664906799793245\n",
      "epoch 6 avg loss: 5.6555634975433355\n",
      "epoch 7 avg loss: 5.4856294631958015\n",
      "epoch 8 avg loss: 5.3436616778373725\n",
      "epoch 9 avg loss: 5.2204542994499215\n",
      "epoch 10 avg loss: 5.1105767369270325\n",
      "epoch 11 avg loss: 5.0105303943157245\n",
      "epoch 12 avg loss: 4.9183861732482915\n",
      "epoch 13 avg loss: 4.8325890243053445\n",
      "epoch 14 avg loss: 4.7522506237030035\n",
      "epoch 15 avg loss: 4.6761632800102245\n",
      "epoch 16 avg loss: 4.6035708576440815\n",
      "epoch 17 avg loss: 4.5343680679798135\n",
      "epoch 18 avg loss: 4.4683866947889335\n",
      "epoch 19 avg loss: 4.4051617622375495\n",
      "epoch 20 avg loss: 4.3449637979269035\n",
      "epoch 21 avg loss: 4.2878345817327555\n",
      "epoch 22 avg loss: 4.2329756230115895\n",
      "epoch 23 avg loss: 4.1799312323331845\n",
      "epoch 24 avg loss: 4.1291833281517025\n",
      "epoch 25 avg loss: 4.0809933781623844\n",
      "epoch 26 avg loss: 4.0340838760137564\n",
      "epoch 27 avg loss: 3.9865567594766618\n",
      "epoch 28 avg loss: 3.9392714768648156\n",
      "epoch 29 avg loss: 3.8936042219400404\n",
      "epoch 30 avg loss: 3.8510383278131486\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(ids_dict)\n",
    "dualmodel_init = DualModel(vocab_size=vocab_size)\n",
    "gpu = 'cuda:3'\n",
    "\n",
    "dualmodel = train_dualmodel(train_xy, model=dualmodel_init, epochs=30, device=gpu, model_fn='dualmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0549f45",
   "metadata": {},
   "source": [
    "### Part 3 - Analysis (5 points)\n",
    "You now have three models.  The original word segmentation model, a sentence generation model, and a dual sentence-generation/word segmentation model. \n",
    "\n",
    "Compare the performance on the test data of the original word segmentation model between the original objective and the dual objective model.  In how many iterations do the models converge?  What are their final F1 and accuracy scores once they've converged? Are they any different?  If so, why?\n",
    "\n",
    "Make the same comparison between the sentence generation model and the dual-objective model, except the performance measure is the per-word perplexity on the text corpus.\n",
    "\n",
    "Report your findings in one of the notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fa85fae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DualModel(\n",
       "  (embedding): Embedding(3650, 200, padding_idx=0)\n",
       "  (lstm): LSTM(200, 150, num_layers=3, batch_first=True, dropout=0.2)\n",
       "  (sig1): Sigmoid()\n",
       "  (lin_to_binary): Linear(in_features=150, out_features=2, bias=True)\n",
       "  (softmax): LogSoftmax(dim=2)\n",
       "  (lin_to_v): Linear(in_features=150, out_features=3650, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dualmodel = torch.load('dualmodel').to('cpu')\n",
    "dualmodel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead403a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Segmenter vs DualModel (output1 only) & compare accuracy/f1\n",
    "\n",
    "def train_and_eval(modelA, modelB, epochs, trainXY, testXY, device, batch_size=50, lr=0.001):\n",
    "    \n",
    "    # Train/test batches\n",
    "    train_batches = DataLoader(train_xy, batch_size=batch_size, shuffle=True)\n",
    "    test_batches = DataLoader(test_xy, batch_size=batch_size)\n",
    "    \n",
    "    # Models\n",
    "    modelA.to(device)\n",
    "    modelB.to(device)\n",
    "    \n",
    "    # Loss funcs & optimizers\n",
    "    cross_entropy_fn = nn.CrossEntropyLoss(ignore_index=0) # ignore sentence padding\n",
    "    nll_fn = nn.NLLLoss(ignore_index=-1) # ignore y padding\n",
    "    optimizerA = optim.Adam(modelA.parameters(), lr=lr)\n",
    "    optimizerB = optim.Adam(modelB.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    # Each epoch: train models -> eval models, print accuracy and f1\n",
    "    for e in range(epochs):\n",
    "        hidden = modelB.init_hidden(batch_size) # the initial random hidden state\n",
    "        \n",
    "        # TRAIN\n",
    "        modelA.train()\n",
    "        modelB.train()\n",
    "        for i, (x, x_len, y) in enumerate(train_batches):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            # ====Train A==========================================\n",
    "            optimizerA.zero_grad()\n",
    "            \n",
    "            out = modelA(x, x_len) \n",
    "            expect = y[:, :max(x_len)] \n",
    "            \n",
    "            lossA = nll_fn(out.permute(0,2,1), expect) \n",
    "            lossA.backward()\n",
    "            optimizerA.step()\n",
    "            \n",
    "            \n",
    "            # ====Train B==========================================\n",
    "            optimizerB.zero_grad()\n",
    "            \n",
    "            hidden = tuple([s.data for s in hidden])\n",
    "            out1, out2 = modelB(x[:,:max(x_len)], hidden)\n",
    "            \n",
    "            # output1 loss\n",
    "            out1 = out1[:, :max(x_len), :] \n",
    "            expect1 = y[:, :max(x_len)] \n",
    "            loss1 = nll_fn(out1.transpose(1, 2), expect1)\n",
    "            # output2 loss\n",
    "            pred, hidden = out2\n",
    "            expect2 = x[:, 1:max(x_len)+1] # next words\n",
    "            loss2 = cross_entropy_fn(pred.transpose(1, 2), expect2)\n",
    "\n",
    "            lossB = loss1+loss2\n",
    "            lossB.backward()\n",
    "            nn.utils.clip_grad_norm_(modelB.parameters(), 1)\n",
    "            optimizerB.step()\n",
    "            \n",
    "        \n",
    "        # TEST\n",
    "        modelA.eval()\n",
    "        modelB.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (x, x_len, y) in enumerate(test_batches):\n",
    "                x, y = x.to(device), y.to(device) # input-sentence, gold-labels\n",
    "                \n",
    "                # ModelA predictions => compare with y\n",
    "                raw_predsA = modelA(x, x_len)\n",
    "                binary_predsA = raw_predsA.argmax(2)\n",
    "                \n",
    "                # ModelB predictions => compare with y\n",
    "                hidden = modelB.init_hidden(batch_size)\n",
    "                raw_predsB, _ = modelB(x[:,:max(x_len)], hidden) # only need the first output\n",
    "                binary_predsB = raw_predsB.argmax(2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
